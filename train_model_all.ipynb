{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87534301",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb753f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import import_ipynb\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR,ReduceLROnPlateau\n",
    "# from models.resnet import ResNet18, ResNet34, ResNet50\n",
    "from models.resnet_soyeong import ResNet34, ResNet18, ResNet50\n",
    "from models.densenet import DenseNet3\n",
    "import torch\n",
    "# import dataloader # custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b4d9e",
   "metadata": {},
   "source": [
    "## Checking and setting CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "log_file = \"./training_logs.txt\"\n",
    " \n",
    "def eval_accuracy(model,log_file ,m_name,id_name, loader, test_data_name = \" \", device = \"cuda\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(f'{test_data_name} Test Accuracy ({m_name,id_name}): {100 * correct / total:.2f}% \\n')\n",
    "    print(f'{test_data_name} Test Accuracy ({m_name,id_name}): {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10775d",
   "metadata": {},
   "source": [
    "## Get DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefe1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset, TensorDataset, RandomSampler\n",
    "from torchvision import datasets\n",
    "def get_loader(dataset,batch_size,id_name,train = False):\n",
    "    '''\n",
    "    dataset = name of dataset for which loader is required\\\n",
    "    batch_size = batch size of the dataloader\n",
    "    id = indistribution data.\n",
    "\n",
    "    output -> \n",
    "    train_loader , testloader (RGB)\n",
    "    '''\n",
    "    data_root='./data' \n",
    "    print(f\"loader requested for {dataset}, to be used on model trained for {id_name} \")\n",
    "    # Transformations\n",
    "    #tranformations for OOD and ADV samples:\n",
    "    # Transformations for Id dataset (training dataset)\n",
    "\n",
    "\n",
    "    '''\n",
    "    id = cifar 10  tranform = cifar10\n",
    "    ood= mnist tranform = cifar10\n",
    "\n",
    "\n",
    "    id = mnist tranform =mnist\n",
    "    ood= = cifar10 tranform = mnist\n",
    "    '''\n",
    "    if id_name == 'svhn':\n",
    "        input_size = (32, 32) \n",
    "        transform_id = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.Resize(input_size), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Common for SVHN\n",
    "        ])\n",
    "    elif id_name == 'cifar10':\n",
    "        input_size = (32, 32) \n",
    "        transform_id  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))   \n",
    "        ])\n",
    "    elif id_name in [ 'mnist','fmnist','kmnist','qmnist']:\n",
    "        input_size = (32, 32) \n",
    "        transform_id=  transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    elif id_name == 'cifar100':\n",
    "        input_size = (32, 32) \n",
    "        transform_id  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))   \n",
    "        ])\n",
    "    if train == True:\n",
    "        datasets_id = {\n",
    "            'cifar10': (\n",
    "                datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'cifar100': (\n",
    "                datasets.CIFAR100(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.CIFAR100(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'mnist': (\n",
    "                datasets.MNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.MNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'qmnist': (\n",
    "                datasets.QMNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.QMNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'kmnist': (\n",
    "                datasets.KMNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.KMNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'fmnist': (\n",
    "                datasets.FashionMNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.FashionMNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'svhn': (\n",
    "                datasets.SVHN(root=data_root, split='train', download=True, transform=transform_id),\n",
    "                datasets.SVHN(root=data_root, split='test', download=True, transform=transform_id)\n",
    "            ),\n",
    "        }\n",
    "        print(\" training dataloaders requested . Tranform = \")\n",
    "         \n",
    "        train_dataset, test_dataset = datasets_id[id_name]\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "        # print(test_loader.dataset.transform)\n",
    "        # print(\" below is the tranform for trainloader\")\n",
    "        # print(train_loader.dataset.transform)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    \n",
    "    print(\" testing loader requested. Transform = \")\n",
    "    if id_name == 'svhn':\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw =  transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Common for SVHN\n",
    "        ])\n",
    "    elif id_name == 'cifar10':\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))   \n",
    "        ])\n",
    "    elif id_name in [ 'mnist','fmnist','kmnist','qmnist']:\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw=  transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    elif id_name == 'cifar100':\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))   \n",
    "        ])\n",
    "\n",
    "    \n",
    "    datasets_ood = {\n",
    "        'cifar10': (\n",
    "            datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'cifar100': (\n",
    "            datasets.CIFAR100(root=data_root, train=False, download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'mnist': (          \n",
    "            datasets.MNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'qmnist': (           \n",
    "            datasets.QMNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'kmnist': (             \n",
    "            datasets.KMNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'fmnist': (         \n",
    "            datasets.FashionMNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'svhn': (\n",
    "            datasets.SVHN(root=data_root, split='test', download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'eurosat': (\n",
    "            datasets.EuroSAT(root=data_root, download=True, transform=transform_rgb)   # same for test (no separate split)\n",
    "        ),\n",
    "        'fake_data_set': (\n",
    "             datasets.FakeData(image_size=(3, 32, 32), num_classes=10, transform=transform_rgb, train=False)\n",
    "        ),\n",
    "        'isun': (\n",
    "              datasets.ImageFolder(root=f\"{data_root}/iSUN\", transform=transform_rgb)\n",
    "        ),\n",
    "        'lsun': (\n",
    "             datasets.LSUN(root=f\"{data_root}/lsun_resize\", transform=transform_rgb)\n",
    "        ),\n",
    "        'dtd': (\n",
    "            datasets.DTD(root=data_root, split='train', download=True, transform=transform_rgb),\n",
    "            datasets.DTD(root=data_root, split='test', download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'places365': (\n",
    "            datasets.Places365(root=data_root, split='val', download=True, transform=transform_rgb)\n",
    "        )\n",
    "        # 'inaturalist': (\n",
    "        #     datasets.INaturalist(root=data_root, version='2021_train', download=False, transform=transform),  # requires download\n",
    "        #     datasets.INaturalist(root=data_root, version='2021_val', download=True, transform=transform)\n",
    "        # )\n",
    "    }\n",
    "    \n",
    "   \n",
    "    \n",
    "    test_dataset = datasets_ood[id_name]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=4,)\n",
    "    print(test_loader.dataset.transform)\n",
    "    return test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18781a",
   "metadata": {},
   "source": [
    "## Target Accuracy for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8d3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = {\n",
    "    \"resnet18\": {\n",
    "        \"svhn\": 96.2,\n",
    "        \"mnist\": 99.2,\n",
    "        \"kmnist\": 99.2,\n",
    "        \"fmnist\": 99.2,\n",
    "        \"qmnist\": 99.2,\n",
    "        \"cifar10\": 94.5,\n",
    "        \"cifar100\": 76.4\n",
    "    },\n",
    "    \"resnet34\": {\n",
    "        \"svhn\": 96.4,\n",
    "        \"mnist\": 99.3,\n",
    "        \"kmnist\": 99.3,\n",
    "        \"fmnist\": 99.3,\n",
    "        \"qmnist\": 99.3,\n",
    "        \"cifar10\": 95.2,\n",
    "        \"cifar100\": 77.8\n",
    "    },\n",
    "    \"resnet50\": {\n",
    "        \"svhn\": 96.5,\n",
    "        \"mnist\": 99.4,\n",
    "        \"kmnist\": 99.4,\n",
    "        \"fmnist\": 99.4,\n",
    "        \"qmnist\": 99.4,\n",
    "        \"cifar10\": 95.6,\n",
    "        \"cifar100\": 78.5\n",
    "    },\n",
    "    \"densenet3\": {\n",
    "        \"svhn\": 96.8,\n",
    "        \"mnist\": 99.5,\n",
    "        \"kmnist\": 99.5,\n",
    "        \"fmnist\": 99.5,\n",
    "        \"qmnist\": 99.5,\n",
    "        \"cifar10\": 96,\n",
    "        \"cifar100\": 79.5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a04b6",
   "metadata": {},
   "source": [
    "## Train with SGD optimizer and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44153103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_setup(model_name,num_classes,device):\n",
    "      # Model setup\n",
    "    if model_name == 'resnet18':\n",
    "        model = ResNet18(num_c=num_classes).to(device)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = ResNet34(num_c=num_classes).to(device)  \n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50(num_classes).to(device)          \n",
    "    elif model_name == 'densenet3':\n",
    "        model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "    return model\n",
    "\n",
    "def train_sgd(model_name='', dataset='', num_epochs=200, batch_size=128, w_decay= 5e-4,log= log_file):        \n",
    "    # Dataset setup\n",
    "    if model_name == \"densenet3\":\n",
    "        num_epochs = 300\n",
    "    ten_classes = ['cifar10','svhn','mnist','fmnist']\n",
    "    if dataset == 'cifar10' or dataset in ten_classes:\n",
    "        num_classes = 10\n",
    "    elif dataset == 'cifar100':  \n",
    "        num_classes = 100  \n",
    "            \n",
    "    save_path = f'./pretrained/SGD/{model_name}_{dataset}/'\n",
    "    \n",
    "    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "    trainloader, test_loader = get_loader( dataset=dataset, batch_size=batch_size,id_name=dataset,train=True)\n",
    "    \n",
    "\n",
    "    model = model_setup(model_name,num_classes,device)\n",
    "\n",
    "    \n",
    "    if os.path.exists(weights_save_path):\n",
    "        print(f\" found weights for {model_name} {dataset} \")\n",
    "        model.load_state_dict(torch.load(weights_save_path, weights_only=True))\n",
    "\n",
    "        # Testing\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        correct = correct*100/ total\n",
    "        diff = abs(correct - accuracy_dict[model_name][dataset])\n",
    "        print(f\" accuracy =  {correct} , with diff = {diff}\")\n",
    "        if diff <1 and dataset == \"cifar100\":\n",
    "            print(\"not training \")\n",
    "            return\n",
    "        if diff < 0.5:\n",
    "            print(\" Not Training \")\n",
    "            return\n",
    "        \n",
    "    \n",
    "    model = model_setup(model_name,num_classes,device)\n",
    "    model.train()\n",
    "     \n",
    "    # Scheduler and loss function setup\n",
    "    milestones = [int(num_epochs * 0.5), int(num_epochs * 0.75)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=0.1, weight_decay=w_decay)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "\n",
    "    running_loss = 0\n",
    "    model.to(device)\n",
    "    epoch_count = 0\n",
    "    with open(log_file,'a') as file:\n",
    "        while epoch_count < num_epochs:\n",
    "            epoch_count += 1\n",
    "        # for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0  # Accumulated loss for one epoch\n",
    "           \n",
    "            model.train()  # Set model to training mode\n",
    "            print(f\" running epoch  =  epoch = {epoch_count}\")\n",
    "            file.write(f\" running epoch  =  epoch = {epoch_count} \\n\")\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # Load data and labels\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass and loss computation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backward pass and weight update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Accumulate epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "            # Accumulate running loss at the end of the epoch\n",
    "            running_loss += epoch_loss\n",
    "            # Update scheduler at the end of each epoch\n",
    "            scheduler.step()\n",
    "            # Print loss and save model every 10 epochs\n",
    "            if (epoch_count + 1) % 5 == 0:\n",
    "                # Compute average loss over 10 epochs\n",
    "                avg_loss = running_loss / (5 * len(trainloader))\n",
    "                print(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.9f}')\n",
    "                file.write(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.9f} \\n\\n')\n",
    "                running_loss = 0.0  # Reset running loss\n",
    "                # --- Accuracy Evaluation on Test Set ---\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():  # Disable gradient computation\n",
    "                    for data in test_loader:  # Assuming you have a testloader\n",
    "                        images, labels = data\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "                weights_save_path = save_path + f'{model_name}_{dataset}_epoch_{epoch_count + 1}.pth'\n",
    "                if accuracy < accuracy_dict[model_name][dataset] and epoch_count + 10 > num_epochs:\n",
    "                    num_epochs+= 50\n",
    "                if accuracy  >= accuracy_dict[model_name][dataset] :\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                    if not os.path.exists(save_path):\n",
    "                        os.makedirs(save_path)\n",
    "                    # Save the model                \n",
    "                    torch.save(model.state_dict(), weights_save_path)\n",
    "                    print(f'Model saved at epoch {epoch_count + 1} in folder {save_path}')\n",
    "                    file.write(f'Model saved at epoch {epoch_count + 1} in folder {save_path} \\n')\n",
    "                    print('Finished Training')\n",
    "                    return \n",
    "\n",
    "                print(f' Accuracy: {accuracy:.2f}% ({correct}/{total}),  epoch total = {num_epochs} , current epoch ={epoch_count}')\n",
    "                file.write(f' Accuracy: {accuracy:.2f}% ({correct}/{total}),  epoch total = {num_epochs} , current epoch ={epoch_count} \\n')\n",
    "                # Create 'pretrained' folder if it doesn't exist\n",
    "                if (epoch_count + 1) % 20 == 0:\n",
    "                    if not os.path.exists(save_path):\n",
    "                        os.makedirs(save_path)\n",
    "                    # Save the model                \n",
    "                    torch.save(model.state_dict(), weights_save_path)\n",
    "                    print(f'Model saved at epoch {epoch_count + 1} in folder {save_path}')\n",
    "                    file.write(f'Model saved at epoch {epoch_count + 1} in folder {save_path} \\n')\n",
    "\n",
    "            \n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af401b",
   "metadata": {},
   "source": [
    "## Train with ADAm optimizer and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eecc8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adam(model_name='', dataset='', num_epochs=200, batch_size=128, w_decay= 5e-4):        \n",
    "    # Dataset setup\n",
    "    ten_classes = ['cifar10','svhn','mnist','fmnist']\n",
    "    if dataset == 'cifar10' or dataset in ten_classes:\n",
    "        num_classes = 10\n",
    "    elif dataset == 'cifar100':  \n",
    "        num_classes = 100  \n",
    "            \n",
    "    save_path = f'./pretrained/Adam/{model_name}_{dataset}/'\n",
    "    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "\n",
    "    if os.path.exists(weights_save_path):\n",
    "        print(f\" found trained model weights for {model_name} , {dataset} \")\n",
    "        model.load_state_dict(torch.load(weights_save_path, weights_only=True))\n",
    "\n",
    "        # Testing\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        correct = correct*100/ total\n",
    "        diff = abs(correct - accuracy_dict[model_name][dataset])\n",
    "        print(f\" accuracy =  {correct} , with diff = {diff}\")\n",
    "        if diff <1 and dataset == \"cifar100\":\n",
    "            print(\"not training \")\n",
    "            return\n",
    "        if diff < 0.5:\n",
    "            print(\" Not Training \")\n",
    "            return\n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\" adam training \\n \\n\")\n",
    "\n",
    "    \n",
    "    trainloader, test_loader = get_loader( dataset=dataset, batch_size=batch_size,id_name=dataset,train=True)\n",
    "\n",
    "    # Model setup\n",
    "    if model_name == 'resnet18':\n",
    "        model = ResNet18(num_c=num_classes).to(device)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = ResNet34(num_c=num_classes).to(device)  \n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50(num_classes).to(device)          \n",
    "    elif model_name == 'densenet3':\n",
    "        model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "    \n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\"model loaded and loader created\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.to(device)\n",
    "    epoch_count = 0\n",
    "    val_loss = 0.0\n",
    "    running_loss = 0\n",
    "    with open(log_file,'a') as file:\n",
    "        while epoch_count < num_epochs:\n",
    "            epoch_count += 1\n",
    "        # for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0  # Accumulated loss for one epoch\n",
    "            \n",
    "            model.train()  # Set model to training mode\n",
    "            print(f\" running epoch  =  epoch = {epoch_count}\")\n",
    "            file.write(f\" running epoch  =  epoch = {epoch_count} \\n\")\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # Load data and labels\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass and loss computation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and weight update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Accumulate running loss at the end of the epoch\n",
    "            running_loss += epoch_loss\n",
    "            \n",
    "            # Print loss and save model every 10 epochs\n",
    "            if (epoch_count + 1) % 10 == 0:\n",
    "                # Compute average loss over 10 epochs\n",
    "                avg_loss = running_loss / (10 * len(trainloader))\n",
    "                print(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.8f}')\n",
    "                file.write(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.8f}\\n')\n",
    "                \n",
    "                running_loss = 0.0  # Reset running loss\n",
    "                # --- Accuracy Evaluation on Test Set ---\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():  # Disable gradient computation\n",
    "                    for data in test_loader:  # Assuming you have a testloader\n",
    "                        images, labels = data\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "                        val_loss += criterion(outputs, labels).item()\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                weights_save_path = save_path + f'{model_name}_{dataset}_epoch_{epoch_count + 1}.pth'\n",
    "                if accuracy < accuracy_dict[model_name][dataset] and epoch_count + 10 > num_epochs:\n",
    "                    num_epochs+= 20\n",
    "                if accuracy+1 >= accuracy_dict[model_name][dataset] and epoch_count == num_epochs and dataset == \"Cifar100\":\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                if accuracy >= accuracy_dict[model_name][dataset] and epoch_count == num_epochs:\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                print(f' Accuracy: {accuracy:.2f}% ({correct}/{total})')\n",
    "                file.write(f' Accuracy: {accuracy:.2f}% ({correct}/{total}),  epoch total = {num_epochs} , current epoch ={epoch_count} \\n')\n",
    "\n",
    "                # Create 'pretrained' folder if it doesn't exist\n",
    "\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "\n",
    "                torch.save(model.state_dict(), weights_save_path)\n",
    "                print(f'Model saved at epoch {epoch_count + 1} in folder {save_path}')\n",
    "            \n",
    "            # scheduler.step()\n",
    "            \n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521747d",
   "metadata": {},
   "source": [
    "## train using RMSPROP and ADAMW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0461eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_RmsProp(model_name='', dataset='', num_epochs=200, batch_size=128):        \n",
    "    # Dataset setup\n",
    "    ten_classes = ['cifar10','svhn','mnist','fmnist']\n",
    "    if dataset == 'cifar10' or dataset in ten_classes:\n",
    "        num_classes = 10\n",
    "    elif dataset == 'cifar100':  \n",
    "        num_classes = 100  \n",
    "            \n",
    "    save_path = f'./pretrained/RMSProp/{model_name}_{dataset}/'\n",
    "    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "\n",
    "    if os.path.exists(weights_save_path):\n",
    "        print(f\" found trained model weights for {model_name} , {dataset} \")\n",
    "        model.load_state_dict(torch.load(weights_save_path, weights_only=True))\n",
    "\n",
    "        # Testing\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        correct = correct*100/ total\n",
    "        diff = abs(correct - accuracy_dict[model_name][dataset])\n",
    "        print(f\" accuracy =  {correct} , with diff = {diff}\")\n",
    "        if diff <1 and dataset == \"cifar100\":\n",
    "            print(\"not training \")\n",
    "            return\n",
    "        if diff < 0.5:\n",
    "            print(\" Not Training \")\n",
    "            return\n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\" adam training \\n \\n\")\n",
    "\n",
    "    \n",
    "    trainloader, test_loader = get_loader( dataset=dataset, batch_size=batch_size,id_name=dataset,train=True)\n",
    "\n",
    "    # Model setup\n",
    "    if model_name == 'resnet18':\n",
    "        model = ResNet18(num_c=num_classes).to(device)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = ResNet34(num_c=num_classes).to(device)  \n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50(num_classes).to(device)          \n",
    "    elif model_name == 'densenet3':\n",
    "        model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "    \n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\"model loaded and loader created\")\n",
    "    # model.apply(lambda m: m.reset_parameters()) \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)\n",
    "\n",
    "    model.to(device)\n",
    "    epoch_count = 0\n",
    "    val_loss = 0.0\n",
    "    running_loss = 0\n",
    "    initial_num_epochs = copy.deepcopy(num_epochs)\n",
    "    with open(log_file,'a') as file:\n",
    "        while epoch_count < num_epochs:\n",
    "            epoch_count += 1\n",
    "        # for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0  # Accumulated loss for one epoch\n",
    "            \n",
    "            model.train()  # Set model to training mode\n",
    "            print(f\" running epoch  =  epoch = {epoch_count}\")\n",
    "            file.write(f\" running epoch  =  epoch = {epoch_count} \\n\")\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # Load data and labels\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass and loss computation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backward pass and weight update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Accumulate epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "            # Accumulate running loss at the end of the epoch\n",
    "            running_loss += epoch_loss            \n",
    "            # Print loss and save model every 10 epochs\n",
    "            if (epoch_count + 1) % 10 == 0:\n",
    "                # Compute average loss over 10 epochs\n",
    "                avg_loss = running_loss / (10 * len(trainloader))\n",
    "                print(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.8f}')\n",
    "                file.write(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.8f}\\n')                \n",
    "                running_loss = 0.0  # Reset running loss\n",
    "                # --- Accuracy Evaluation on Test Set ---\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():  # Disable gradient computation\n",
    "                    for data in test_loader:  # Assuming you have a testloader\n",
    "                        images, labels = data\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "                        val_loss += criterion(outputs, labels).item()\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                weights_save_path = save_path + f'{model_name}_{dataset}_epoch_{epoch_count + 1}.pth'\n",
    "                if accuracy < accuracy_dict[model_name][dataset] and epoch_count + 10 > num_epochs:\n",
    "                    num_epochs+= 20\n",
    "                if accuracy+1 >= accuracy_dict[model_name][dataset] and epoch_count == num_epochs and dataset == \"Cifar100\":\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                if accuracy >= accuracy_dict[model_name][dataset] and epoch_count == num_epochs:\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                if num_epochs-initial_num_epochs >200:\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                print(f' Accuracy: {accuracy:.2f}% ({correct}/{total})')\n",
    "                file.write(f' Accuracy: {accuracy:.2f}% ({correct}/{total}),  epoch total = {num_epochs} , current epoch ={epoch_count} \\n')\n",
    "                # Create 'pretrained' folder if it doesn't exist\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "\n",
    "                torch.save(model.state_dict(), weights_save_path)\n",
    "                print(f'Model saved at epoch {epoch_count + 1} in folder {save_path}')\n",
    "            \n",
    "            # scheduler.step()\n",
    "            \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_AdamW(model_name='', dataset='', num_epochs=200, batch_size=128):        \n",
    "    # Dataset setup\n",
    "    \n",
    "    ten_classes = ['cifar10','svhn','mnist','fmnist']\n",
    "    if dataset == 'cifar10' or dataset in ten_classes:\n",
    "        num_classes = 10\n",
    "    elif dataset == 'cifar100':  \n",
    "        num_classes = 100  \n",
    "            \n",
    "    save_path = f'./pretrained/AdamW/{model_name}_{dataset}/'\n",
    "    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "\n",
    "    if os.path.exists(weights_save_path):\n",
    "        print(f\" found trained model weights for {model_name} , {dataset} \")\n",
    "        model.load_state_dict(torch.load(weights_save_path, weights_only=True))\n",
    "\n",
    "        # Testing\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        correct = correct*100/ total\n",
    "        diff = abs(correct - accuracy_dict[model_name][dataset])\n",
    "        print(f\" accuracy =  {correct} , with diff = {diff}\")\n",
    "        if diff <1 and dataset == \"cifar100\":\n",
    "            print(\"not training \")\n",
    "            return\n",
    "        if diff < 0.5:\n",
    "            print(\" Not Training \")\n",
    "            return\n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\" adam training \\n \\n\")\n",
    "\n",
    "    \n",
    "    trainloader, test_loader = get_loader( dataset=dataset, batch_size=batch_size,id_name=dataset,train=True)\n",
    "\n",
    "    # Model setup\n",
    "    if model_name == 'resnet18':\n",
    "        model = ResNet18(num_c=num_classes).to(device)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = ResNet34(num_c=num_classes).to(device)  \n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50(num_classes).to(device)          \n",
    "    elif model_name == 'densenet3':\n",
    "        num_epochs = 300\n",
    "        model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "    \n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\"model loaded and loader created\")\n",
    "    model.apply(lambda m: m.reset_parameters()) \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, momentum=0.9, weight_decay=1e-5)\n",
    "    initial_num_epochs = copy.deepcopy(num_epochs)\n",
    "    model.to(device)\n",
    "    epoch_count = 0\n",
    "    val_loss = 0.0\n",
    "    running_loss = 0\n",
    "    with open(log_file,'a') as file:\n",
    "        while epoch_count < num_epochs:\n",
    "            epoch_count += 1\n",
    "        # for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0  # Accumulated loss for one epoch\n",
    "            \n",
    "            model.train()  # Set model to training mode\n",
    "            print(f\" running epoch  =  epoch = {epoch_count}\")\n",
    "            file.write(f\" running epoch  =  epoch = {epoch_count} \\n\")\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                # Load data and labels\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass and loss computation\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Backward pass and weight update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Accumulate epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "            # Accumulate running loss at the end of the epoch\n",
    "            running_loss += epoch_loss            \n",
    "            # Print loss and save model every 10 epochs\n",
    "            if (epoch_count + 1) % 10 == 0:\n",
    "                # Compute average loss over 10 epochs\n",
    "                avg_loss = running_loss / (10 * len(trainloader))\n",
    "                print(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.8f}')\n",
    "                file.write(f'Epoch [{epoch_count + 1}] Average Loss: {avg_loss:.8f}\\n')                \n",
    "                running_loss = 0.0  # Reset running loss\n",
    "                # --- Accuracy Evaluation on Test Set ---\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():  # Disable gradient computation\n",
    "                    for data in test_loader:  # Assuming you have a testloader\n",
    "                        images, labels = data\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "                        val_loss += criterion(outputs, labels).item()\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                weights_save_path = save_path + f'{model_name}_{dataset}_epoch_{epoch_count + 1}.pth'\n",
    "                if accuracy < accuracy_dict[model_name][dataset] and epoch_count + 10 > num_epochs:\n",
    "                    num_epochs+= 20\n",
    "                if accuracy+1 >= accuracy_dict[model_name][dataset] and epoch_count == num_epochs and dataset == \"Cifar100\":\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                if accuracy >= accuracy_dict[model_name][dataset] and epoch_count == num_epochs:\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                if num_epochs-initial_num_epochs >200:\n",
    "                    epoch_count = num_epochs\n",
    "                    weights_save_path = save_path + f'{model_name}_{dataset}.pth'\n",
    "                print(f' Accuracy: {accuracy:.2f}% ({correct}/{total})')\n",
    "                file.write(f' Accuracy: {accuracy:.2f}% ({correct}/{total}),  epoch total = {num_epochs} , current epoch ={epoch_count} \\n')\n",
    "                # Create 'pretrained' folder if it doesn't exist\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "\n",
    "                torch.save(model.state_dict(), weights_save_path)\n",
    "                print(f'Model saved at epoch {epoch_count + 1} in folder {save_path}')\n",
    "            \n",
    "            # scheduler.step()\n",
    "            \n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be72051",
   "metadata": {},
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e4360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader requested for cifar10, to be used on model trained for cifar10 \n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      " training dataloaders requested . Tranform = \n",
      " running epoch  =  epoch = 1\n",
      " running epoch  =  epoch = 2\n",
      " running epoch  =  epoch = 3\n"
     ]
    }
   ],
   "source": [
    "models = [\"resnet34\",\"densenet3\",'resnet50',\"resnet18\"]\n",
    "id_names = ['cifar10',\"cifar100\",\"mnist\",\"svhn\",\"fmnist\",\"kmnist\",\"qmnist\"]\n",
    " \n",
    "# with open(log_file,\"a\") as file:\n",
    "#     for id_name in id_names:\n",
    "#         for model in models:\n",
    "#             file.write(\"-----------------------------------------------\\n\\n\")\n",
    "                \n",
    "#             file.write(f\" processing {model} and {id_name}\\n\")\n",
    "#             train_sgd(model,id_name)\n",
    "#             file.write(\"SGD done  ------\\n\\n\")\n",
    "           \n",
    "#         file.write(\"-----------------------------------------------\\n\\n\")\n",
    " \n",
    "# models = [\"resnet18\",\"resnet34\",\"densenet3\",'resnet50']\n",
    "# id_names = ['cifar10',\"cifar100\"]\n",
    "# pair = [(\"resnet50\",\"svhn\"),(\"resnet50\",\"mnist\")]\n",
    "# with open(log_file,\"a\") as file:\n",
    "#     for id_name in id_names:\n",
    "#         for model in models:\n",
    "#             file.write(\"--------------------ADAM ---------------------------\\n\\n\")\n",
    "                \n",
    "#             file.write(f\" processing {model} and {id_name}\\n\")\n",
    "#             train_adam(model,id_name)\n",
    "#             file.write(\"SGD done  ------\\n\\n\")\n",
    "           \n",
    "#         file.write(\"-----------------------------------------------\\n\\n\")\n",
    "\n",
    "#     generate_adversarial_samples(model, test_loader, dataset,, )\n",
    "\n",
    "with open(log_file,\"a\") as file:\n",
    "    for id_name in id_names:\n",
    "        for model in models:\n",
    "            file.write(\"-----------Generating ADV Samples--------------\\n\\n\")\n",
    "            \n",
    "            file.write(f\" processing {model} and {id_name}\\n\")\n",
    "            train_RmsProp(model,id_name)\n",
    "            # train_AdamW(model,id_name)\n",
    "            file.write(\"SGD done  ------\\n\\n\")\n",
    "           \n",
    "        file.write(\"-----------------------------------------------\\n\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4441a",
   "metadata": {},
   "source": [
    "# generate ADV Samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6320652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import torchattacks\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from tqdm import tqdm \n",
    " \n",
    "# def generate_adversarial_samples(model_name,dataset,training_type):\n",
    "#     \"\"\"\n",
    "#     Generate and save adversarial samples using FGSM, PGD, DeepFool, and AutoAttack.\n",
    "    \n",
    "#     Args:\n",
    "#         model: PyTorch model to attack\n",
    "#         test_loader: DataLoader with test data\n",
    "#         m_name: Model name for file naming\n",
    "#         id_name: Identifier for file naming\n",
    "#         device: Device to run the model on ('cuda' or 'cpu')\n",
    "#         training_type = SGD,RMSPRop,Adam,AdamW.\n",
    "#     \"\"\"\n",
    "#     batch_size = 2048\n",
    "#     _, test_loader = get_loader( dataset=dataset, batch_size=batch_size,id_name=dataset,train=True)\n",
    "\n",
    "#     ten_classes = ['cifar10','svhn','mnist','fmnist']\n",
    "#     if dataset == 'cifar10' or dataset in ten_classes:\n",
    "#         num_classes = 10\n",
    "#     elif dataset == 'cifar100':  \n",
    "#         num_classes = 100  \n",
    "\n",
    "#     # Model setup\n",
    "#     if model_name == 'resnet18':\n",
    "#         model = ResNet18(num_c=num_classes).to(device)\n",
    "#     elif model_name == 'resnet34':\n",
    "#         model = ResNet34(num_c=num_classes).to(device)  \n",
    "#     elif model_name == 'resnet50':\n",
    "#         model = ResNet50(num_classes).to(device)          \n",
    "#     elif model_name == 'densenet3':\n",
    "#         model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "    \n",
    "#     with open(log_file,'a') as file:\n",
    "#         file.write(\"model loaded and loader created\")\n",
    "    \n",
    "#     # Initialize attack methods\n",
    " \n",
    "#     attacks = {\n",
    "#         'fgsm': torchattacks.FGSM(model, eps=0.03),\n",
    "#         'pgd': torchattacks.PGD(model, eps=0.03, alpha=0.01, steps=10),\n",
    "#         'deepfool': torchattacks.DeepFool(model, steps=50, overshoot=0.02),\n",
    "#         'autoattack': torchattacks.AutoAttack(model, eps=0.03,version = 'standard',  norm='Linf'),\n",
    "#         'cw' :  torchattacks.CW(model, c=1e-4, kappa=0, steps=1000, lr=0.01)  \n",
    "#     }\n",
    "    \n",
    "#     # Ensure output directory exists\n",
    "#     os.makedirs(f'./adv_samples/{training_type}', exist_ok=True)\n",
    "    \n",
    "#     for attack_name, attack in attacks.items():\n",
    "#         save_path = f'./adv_samples/{training_type}/{training_type}/{model_name}_{dataset}_{attack_name}.pt'\n",
    "#         if os.path.exists(save_path):\n",
    "#             print(\" samples already exist \")\n",
    "#         else :\n",
    "#             with open(log_file, 'a') as file:\n",
    "#                 file.write(f\" Creating {attack_name} samples for  {model_name} {dataset} .. \\n\")\n",
    "#             data_list = []\n",
    "#             label_list = []\n",
    "        \n",
    "#             for data, target in tqdm(test_loader, desc=f\"Generating {attack_name} samples\"):\n",
    "#                 data, target = data.to(device), target.to(device)\n",
    "                \n",
    "#                 # Generate adversarial samples\n",
    "#                 adv_data = attack(data, target)\n",
    "                \n",
    "#                 # Convert to numpy for saving\n",
    "#                 # data_list.extend(adv_data.cpu().numpy())\n",
    "#                 # label_list.extend(target.cpu().numpy())\n",
    "#                 data_list.extend(adv_data.detach().cpu().numpy())\n",
    "#                 label_list.extend(target.detach().cpu().numpy())\n",
    "            \n",
    "#             # Save adversarial samples\n",
    "#             adv_samples = list(zip(data_list, label_list))\n",
    "            \n",
    "#             torch.save(adv_samples, save_path)\n",
    "#             with open(log_file, 'a') as file:\n",
    "#                 file.write(f'Saved {attack_name} adversarial samples to {save_path} \\n')\n",
    "#             print(f'Saved {attack_name} adversarial samples to {save_path}')\n",
    "        \n",
    "#         # Verify saved data can be loaded\n",
    "#         loaded_data = torch.load(save_path, weights_only=False)\n",
    "#         loaded_data_list, loaded_label_list = zip(*loaded_data)\n",
    "#         inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in loaded_data_list])\n",
    "#         labels_tensor = torch.tensor(loaded_label_list, dtype=torch.long)\n",
    "#         adv_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "#         adv_test_loader = DataLoader(adv_dataset, batch_size=test_loader.batch_size, shuffle=False,num_workers=4)\n",
    "#         eval_accuracy(model,log_file,model_name,dataset,adv_test_loader,attack_name)\n",
    "#         print(f'Verified loading for {attack_name} dataset: {len(adv_dataset)} samples')\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b532f7",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412278bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model_name='', id_dataset='', attack='', ood_dataset='', batch_size=0):\n",
    "#     # Dataset setup\n",
    "#     if id_dataset == 'cifar10':\n",
    "#         num_classes = 10\n",
    "#     elif id_dataset == 'cifar100':\n",
    "#         num_classes = 100  # Regular CIFAR-100 has 100 fine classes\n",
    "            \n",
    "#     mean, std = dataloader.get_mean_std(dataset=id_dataset)\n",
    "            \n",
    "#     if attack == '' and ood_dataset == '':                \n",
    "#         print(f'=========================== Test model for {model_name}_{id_dataset} ==================================')\n",
    "#         _, testloader = dataloader.get_imageloader(dataset=id_dataset, batch_size=batch_size, mean=mean, std=std)        \n",
    "#     elif attack != '':\n",
    "#         print(f'=========================== Test model for {model_name}_{attack} ==================================')\n",
    "#         _, testloader = dataloader.get_imageloader(model_name=model_name, dataset=f'{id_dataset}_{attack}', batch_size=batch_size, mean=mean, std=std)\n",
    "#     elif ood_dataset != '':                \n",
    "#         print(f'=========================== Test model for {model_name}_{ood_dataset} ==================================')\n",
    "#         _, testloader = dataloader.get_imageloader(dataset=id_dataset, batch_size=batch_size, mean=mean, std=std)\n",
    "    \n",
    "#     # Model setup\n",
    "#     if model_name == 'resnet18':\n",
    "#         model = ResNet18(num_c=num_classes).to(device)\n",
    "#     elif model_name == 'resnet34':\n",
    "#         model = ResNet34(num_c=num_classes).to(device)  \n",
    "#     elif model_name == 'resnet50':\n",
    "#         model = ResNet50(num_c=num_classes).to(device)          \n",
    "#     elif model_name == 'densenet3':\n",
    "#         model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "\n",
    "#     # Load model weights (from the last epoch)\n",
    "#     model_path = f'./pretrained/{model_name}_{id_dataset}.pth'\n",
    "#     model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "#     # Testing\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data in testloader:\n",
    "#             images, labels = data\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     # Print accuracy\n",
    "#     print(f'Accuracy of the network on the {total} test images: {100 * correct / total:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
