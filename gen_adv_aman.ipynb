{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f427ed",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR,ReduceLROnPlateau\n",
    "# from models.resnet import ResNet18, ResNet34, ResNet50\n",
    "from models.resnet_soyeong import ResNet34, ResNet18, ResNet50\n",
    "from models.densenet import DenseNet3\n",
    "import torch\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, RandomSampler\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9678dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "device = \"cuda:0\"\n",
    "log_file = \"./logs/train_adv_gen_logs.txt\"\n",
    "\n",
    "def eval_accuracy(model,log_file ,m_name,id_name, loader, test_data_name = \" \", device = device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(f'{test_data_name} Test Accuracy ({m_name,id_name}): {accuracy:.2f}% \\n')\n",
    "    print(f'{test_data_name} Test Accuracy ({m_name,id_name}): {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d648df7",
   "metadata": {},
   "source": [
    "## Get DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe3f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loader(dataset,batch_size,id_name,train = False):\n",
    "    '''\n",
    "    dataset = name of dataset for which loader is required\\\n",
    "    batch_size = batch size of the dataloader\n",
    "    id = indistribution data.\n",
    "\n",
    "    output -> \n",
    "    train_loader , testloader (RGB)\n",
    "    '''\n",
    "    data_root='./data' \n",
    "    print(f\"loader requested for {dataset}, to be used on model trained for {id_name} \")\n",
    "    # Transformations\n",
    "    #tranformations for OOD and ADV samples:\n",
    "    # Transformations for Id dataset (training dataset)\n",
    "\n",
    "\n",
    "    '''\n",
    "    id = cifar 10  tranform = cifar10\n",
    "    ood= mnist tranform = cifar10\n",
    "\n",
    "\n",
    "    id = mnist tranform =mnist\n",
    "    ood= = cifar10 tranform = mnist\n",
    "    '''\n",
    "    if id_name == 'svhn':\n",
    "        input_size = (32, 32) \n",
    "        transform_id = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            # transforms.RandomCrop(32, padding=4),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.Resize(input_size), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Common for SVHN\n",
    "        ])\n",
    "    elif id_name == 'cifar10':\n",
    "        input_size = (32, 32) \n",
    "        transform_id  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            # transforms.RandomCrop(32, padding=4),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))   \n",
    "        ])\n",
    "    elif id_name in [ 'mnist','fmnist','kmnist','qmnist']:\n",
    "        input_size = (32, 32) \n",
    "        transform_id=  transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            # transforms.RandomCrop(32, padding=4),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    elif id_name == 'cifar100':\n",
    "        input_size = (32, 32) \n",
    "        transform_id  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            # transforms.RandomCrop(32, padding=4),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))   \n",
    "        ])\n",
    "    if train == True:\n",
    "        datasets_id = {\n",
    "            'cifar10': (\n",
    "                datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'cifar100': (\n",
    "                datasets.CIFAR100(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.CIFAR100(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'mnist': (\n",
    "                datasets.MNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.MNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'qmnist': (\n",
    "                datasets.QMNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.QMNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'kmnist': (\n",
    "                datasets.KMNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.KMNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'fmnist': (\n",
    "                datasets.FashionMNIST(root=data_root, train=True, download=True, transform=transform_id),\n",
    "                datasets.FashionMNIST(root=data_root, train=False, download=True, transform=transform_id)\n",
    "            ),\n",
    "            'svhn': (\n",
    "                datasets.SVHN(root=data_root, split='train', download=True, transform=transform_id),\n",
    "                datasets.SVHN(root=data_root, split='test', download=True, transform=transform_id)\n",
    "            ),\n",
    "        }\n",
    "        print(\" training dataloaders requested . Tranform = \")\n",
    "         \n",
    "        train_dataset, test_dataset = datasets_id[id_name]\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "        # print(test_loader.dataset.transform)\n",
    "        # print(\" below is the tranform for trainloader\")\n",
    "        # print(train_loader.dataset.transform)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    \n",
    "    print(\" testing loader requested. Transform = \")\n",
    "    if id_name == 'svhn':\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw =  transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Common for SVHN\n",
    "        ])\n",
    "    elif id_name == 'cifar10':\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))   \n",
    "        ])\n",
    "    elif id_name in [ 'mnist','fmnist','kmnist','qmnist']:\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw=  transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    elif id_name == 'cifar100':\n",
    "        input_size = (32, 32) \n",
    "        transform_rgb = transform_id\n",
    "        transform_bw  = transforms.Compose([\n",
    "            transforms.Resize(input_size), \n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),   \n",
    "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))   \n",
    "        ])\n",
    "\n",
    "    \n",
    "    datasets_ood = {\n",
    "        'cifar10': (\n",
    "            datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'cifar100': (\n",
    "            datasets.CIFAR100(root=data_root, train=False, download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'mnist': (          \n",
    "            datasets.MNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'qmnist': (           \n",
    "            datasets.QMNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'kmnist': (             \n",
    "            datasets.KMNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'fmnist': (         \n",
    "            datasets.FashionMNIST(root=data_root, train=False, download=True, transform=transform_bw)\n",
    "        ),\n",
    "        'svhn': (\n",
    "            datasets.SVHN(root=data_root, split='test', download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'eurosat': (\n",
    "            datasets.EuroSAT(root=data_root, download=True, transform=transform_rgb)   # same for test (no separate split)\n",
    "        ),\n",
    "        'fake_data_set': (\n",
    "             datasets.FakeData(image_size=(3, 32, 32), num_classes=10, transform=transform_rgb, train=False)\n",
    "        ),\n",
    "        'isun': (\n",
    "              datasets.ImageFolder(root=f\"{data_root}/iSUN\", transform=transform_rgb)\n",
    "        ),\n",
    "        'lsun': (\n",
    "             datasets.LSUN(root=f\"{data_root}/lsun_resize\", transform=transform_rgb)\n",
    "        ),\n",
    "        'dtd': (\n",
    "            datasets.DTD(root=data_root, split='train', download=True, transform=transform_rgb),\n",
    "            datasets.DTD(root=data_root, split='test', download=True, transform=transform_rgb)\n",
    "        ),\n",
    "        'places365': (\n",
    "            datasets.Places365(root=data_root, split='val', download=True, transform=transform_rgb)\n",
    "        )\n",
    "        # 'inaturalist': (\n",
    "        #     datasets.INaturalist(root=data_root, version='2021_train', download=False, transform=transform),  # requires download\n",
    "        #     datasets.INaturalist(root=data_root, version='2021_val', download=True, transform=transform)\n",
    "        # )\n",
    "    }\n",
    "    \n",
    "   \n",
    "    \n",
    "    test_dataset = datasets_ood[id_name]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=4,)\n",
    "    print(test_loader.dataset.transform)\n",
    "    return test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea2b7d",
   "metadata": {},
   "source": [
    "## Generate_ADV_Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e565cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchattacks\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm \n",
    " \n",
    "def generate_adversarial_samples(model_name,dataset,training_type):\n",
    "    \"\"\"\n",
    "    Generate and save adversarial samples using FGSM, PGD, DeepFool,CW and AutoAttack.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to attack\n",
    "        Dataset: dataset on which model is trained and adv samples are required\n",
    "        training_type: type of method used to train the model (SGD,ADAM,RMSProp,ADamw)\n",
    "    \"\"\"\n",
    "    batch_size = 4096\n",
    "    _, test_loader = get_loader( dataset=dataset, batch_size=batch_size,id_name=dataset,train=True)\n",
    "\n",
    "    ten_classes = ['cifar10','svhn','mnist','fmnist',\"kmnist\",\"qmnist\"]\n",
    "    if dataset == 'cifar10' or dataset in ten_classes:\n",
    "        num_classes = 10\n",
    "    elif dataset == 'cifar100':  \n",
    "        num_classes = 100\n",
    "    else:\n",
    "        print(\" error dataset \")\n",
    "        exit()\n",
    "    # Model setup\n",
    "    if model_name == 'resnet18':\n",
    "        model = ResNet18(num_c=num_classes).to(device)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = ResNet34(num_c=num_classes).to(device)  \n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50(num_classes).to(device)          \n",
    "    elif model_name == 'densenet3':\n",
    "        model = DenseNet3(100, num_classes, growth_rate=12).to(device)\n",
    "    \n",
    "    with open(log_file,'a') as file:\n",
    "        file.write(\"model loaded and loader created\")\n",
    "    \n",
    "    # Initialize attack methods\n",
    "    os_path = f'/scratch/asing651/mps/Mps'\n",
    "    model_path = os_path +f'/pre_trained/{training_type}/{model_name}_{dataset}/{model_name}_{dataset}.pth'\n",
    "    # print(\"Current Working Directory:\", os.getcwd())\n",
    "    \n",
    "    # if os.path.exists(os_path):\n",
    "    #     print( os_path, \" exists\")\n",
    "        \n",
    "    # else:\n",
    "    #     print( os_path, \" not found\")\n",
    "    #     exit()\n",
    "    # os_path += f'/pre_trained/{training_type}'   \n",
    "    # if os.path.exists(os_path):\n",
    "    #     print( os_path, \" exists\")  \n",
    "    # else:\n",
    "    #     print( os_path, \" not found\")\n",
    "    #     exit()\n",
    "    # os_path += f\"/{model_name}_{dataset}\"\n",
    "    # if os.path.exists(os_path):\n",
    "    #     print(os_path, \" exists \\n\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(model_path, \" exists \\n\")\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    else:\n",
    "        print(\"-------------------------------------\\n\")\n",
    "        print(model_path)\n",
    "        print(f\" no weights found for {model_name},{dataset},{training_type}\")\n",
    "        return\n",
    "    model.eval()\n",
    "    attacks = {\n",
    "        'fgsm': torchattacks.FGSM(model, eps=0.03),\n",
    "        'pgd': torchattacks.PGD(model, eps=0.03, alpha=0.01, steps=10),\n",
    "        'deepfool': torchattacks.DeepFool(model, steps=50, overshoot=0.02),\n",
    "        'autoattack': torchattacks.AutoAttack(model, eps=0.03,version = 'standard',  norm='Linf'),\n",
    "        'cw' :  torchattacks.CW(model, c=1e-4, kappa=0, steps=1000, lr=0.01)  ,\n",
    "\n",
    "        'cw_strong': torchattacks.CW(model, c=0.01, kappa=0, steps=500, lr=0.005),\n",
    "        'autoattack_strong': torchattacks.AutoAttack(model, eps=0.05, version='standard', norm='Linf'),\n",
    "        'deepfool_strong': torchattacks.DeepFool(model, steps=50, overshoot=0.05),\n",
    "        'pgd_strong': torchattacks.PGD(model, eps=0.05, alpha=0.05/40, steps=40),\n",
    "        'fgsm_strong': torchattacks.FGSM(model, eps=0.05)\n",
    "\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(f'./adv_samples/{training_type}', exist_ok=True)\n",
    "   \n",
    "    for attack_name, attack in attacks.items():\n",
    "        save_path = f'./adv_samples/{training_type}/{model_name}_{dataset}_{attack_name}.pt'\n",
    "        temp_name = attack_name[:-7] \n",
    "        if temp_name in attacks:\n",
    "            # Verify saved data can be loaded\n",
    "            \n",
    "            save_path_prev = f'./adv_samples/{training_type}/{model_name}_{dataset}_{temp_name}.pt'\n",
    "            if os.path.exists(save_path_prev) :\n",
    "                loaded_data = torch.load(save_path_prev, weights_only=False)\n",
    "                loaded_data_list, loaded_label_list = zip(*loaded_data)\n",
    "                inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in loaded_data_list])\n",
    "                labels_tensor = torch.tensor(loaded_label_list, dtype=torch.long)\n",
    "                adv_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "                adv_test_loader = DataLoader(adv_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "                accuracy = eval_accuracy(model,log_file,model_name,dataset,adv_test_loader,attack_name)\n",
    "                \n",
    "                print(f'Verified loading for {attack_name} dataset: {len(adv_dataset)} samples')\n",
    "                if accuracy < 25 : \n",
    "                    continue\n",
    "        if os.path.exists(save_path) :\n",
    "            loaded_data = torch.load(save_path, weights_only=False)\n",
    "            loaded_data_list, loaded_label_list = zip(*loaded_data)\n",
    "            inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in loaded_data_list])\n",
    "            labels_tensor = torch.tensor(loaded_label_list, dtype=torch.long)\n",
    "            adv_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "            adv_test_loader = DataLoader(adv_dataset, batch_size=test_loader.batch_size, shuffle=False,num_workers=4)\n",
    "            accuracy = eval_accuracy(model,log_file,model_name,dataset,adv_test_loader,temp_name)\n",
    "            print(\" samples already exist \")\n",
    "            continue\n",
    "        \n",
    "        with open(log_file, 'a') as file:\n",
    "            file.write(f\" Creating {attack_name} samples for  {model_name} {dataset} .. \\n\")\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "    \n",
    "        for data, target in tqdm(test_loader, desc=f\"Generating {attack_name} samples\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Generate adversarial samples\n",
    "            adv_data = attack(data, target)\n",
    "            \n",
    "            # Convert to numpy for saving\n",
    "            # data_list.extend(adv_data.cpu().numpy())\n",
    "            # label_list.extend(target.cpu().numpy())\n",
    "            data_list.extend(adv_data.detach().cpu().numpy())\n",
    "            label_list.extend(target.detach().cpu().numpy())\n",
    "        \n",
    "        # Save adversarial samples\n",
    "        adv_samples = list(zip(data_list, label_list))\n",
    "        \n",
    "        torch.save(adv_samples, save_path)\n",
    "        with open(log_file, 'a') as file:\n",
    "            file.write(f'Saved {attack_name} adversarial samples to {save_path} \\n')\n",
    "        print(f'Saved {attack_name} adversarial samples to {save_path}')\n",
    "    \n",
    "    # Verify saved data can be loaded\n",
    "    loaded_data = torch.load(save_path, weights_only=False)\n",
    "    loaded_data_list, loaded_label_list = zip(*loaded_data)\n",
    "    inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in loaded_data_list])\n",
    "    labels_tensor = torch.tensor(loaded_label_list, dtype=torch.long)\n",
    "    adv_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "    adv_test_loader = DataLoader(adv_dataset, batch_size=test_loader.batch_size, shuffle=False,num_workers=4)\n",
    "    accuracy = eval_accuracy(model,log_file,model_name,dataset,adv_test_loader,attack_name)\n",
    "    print(f\" accuracy of {model_name , dataset, attack_name,training_type } = {accuracy}  \")\n",
    "    print(f'Verified loading for {attack_name} dataset: {len(adv_dataset)} samples')\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd1d31",
   "metadata": {},
   "source": [
    "## Get DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e093b8",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SGD', 'SGD', 'SGD', 'SGD']\n",
      " processing resnet34 and svhn with training optim = SGD\n",
      "loader requested for svhn, to be used on model trained for svhn \n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      " training dataloaders requested . Tranform = \n",
      "/scratch/asing651/mps/Mps/pre_trained/SGD/resnet34_svhn/resnet34_svhn.pth  exists \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating fgsm samples: 100%|██████████| 26/26 [00:07<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fgsm adversarial samples to ./adv_samples/SGD/resnet34_svhn_fgsm.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pgd samples: 100%|██████████| 26/26 [00:52<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pgd adversarial samples to ./adv_samples/SGD/resnet34_svhn_pgd.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating deepfool samples: 100%|██████████| 26/26 [2:22:39<00:00, 329.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deepfool adversarial samples to ./adv_samples/SGD/resnet34_svhn_deepfool.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating autoattack samples:  69%|██████▉   | 18/26 [1:01:22<25:43, 192.96s/it]"
     ]
    }
   ],
   "source": [
    "models = [\"resnet34\",\"densenet3\",'resnet50',\"resnet18\"]\n",
    "id_names = [\"svhn\",\"fmnist\",'cifar10',\"cifar100\",\"mnist\"]\n",
    "\n",
    "optim_types = [\"SGD\"]\n",
    "optim_types += optim_types\n",
    "optim_types += optim_types\n",
    "print(optim_types)\n",
    "with open(log_file,\"a\") as file:\n",
    "    for optim_type in optim_types:\n",
    "        for id_name in id_names:\n",
    "            for model in models:\n",
    "                file.write(\"----------Gen_ADV-------------\\n\\n\")   \n",
    "                file.write(f\" processing {model} and {id_name} with training optim = {optim_type}\\n\")\n",
    "                print(f\" processing {model} and {id_name} with training optim = {optim_type}\")\n",
    "                generate_adversarial_samples(model,id_name,optim_type)   \n",
    "        file.write(\"-----------------------------------------------\\n\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1af3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cdaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
